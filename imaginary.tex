\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{booktabs}

\title{Training Large Language Models in Complex Hilbert Space:\\
Imaginary-Time Gradient Flow and Phase-Locked Transformers}

\author{Payton Ison \\ The Singularity}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) are typically formulated over real vector spaces and trained with Euclidean optimization, despite empirical evidence that their internal computation resembles wave-like propagation and interference of distributed representations. In this work, we propose a formulation of transformer-based LLMs as dynamical systems evolving in a \emph{complex Hilbert space}, and we describe an associated training procedure based on \emph{imaginary-time gradient flow} using Wirtinger calculus. Building on existing work in complex-valued neural networks, unitary recurrent architectures, and complex-valued transformers, we define a \emph{Hilbert-space transformer} in which (i) token embeddings, hidden states, and attention features live in $\mathbb{C}^d$; (ii) core blocks are parameterized as (near-)unitary operators generated by Hermitian matrices; and (iii) optimization is performed with respect to complex-conjugate parameters, yielding updates that jointly shape the \emph{magnitude} and \emph{phase} structure of internal representations. We argue that this formulation provides (1) improved gradient stability through norm-preserving flows, (2) increased expressivity via phase-coded structure and interference, and (3) a natural interpretation of training as energy minimization in a complex landscape, analogous to imaginary-time evolution in quantum mechanics. We further articulate a \emph{Resonant Phase-Locking} hypothesis, in which large-scale training induces phase-locked internal modes that support long-range credit assignment and human-in-the-loop adaptation. We outline concrete architectural choices for complex self-attention, feed-forward blocks, and normalization, and propose experimental protocols for evaluating Hilbert-space transformers on language modeling, long-context reasoning, and interactive alignment.
\end{abstract}

\section{Introduction}

Transformer-based large language models (LLMs) have achieved impressive performance across a wide range of tasks, yet their mathematical formulation remains rooted in real Euclidean vector spaces. Hidden states, parameters, and gradients are typically taken to lie in $\mathbb{R}^d$, and optimization is performed via real-valued gradient descent in this space. 

At the same time, a growing body of empirical work suggests that the internal dynamics of deep sequence models exhibit \emph{wave-like} phenomena: oscillatory modes across depth, interference between attention heads, and phase-like structure induced by positional encodings and Fourier-style features. These observations invite a reformulation in terms of \emph{complex-valued} state spaces and operators, where the language of amplitude, phase, and interference is native.

Parallel research in complex-valued neural networks (CVNNs) has shown that explicitly modeling amplitude and phase can yield improvements on tasks where the data are naturally complex---including speech processing, medical imaging, radar, and wireless communications---provided that both the architecture and the training algorithm are properly formulated in the complex domain. Complex-valued recurrent and unitary networks further demonstrate that constraining dynamics to live on (or near) unitary manifolds in complex Hilbert space can stabilize gradients over long temporal horizons.

More recently, complex-valued transformer variants have been proposed for modulation recognition, channel estimation, and other signal-processing tasks, with promising results. Yet for general-purpose LLMs, complex geometry is typically relegated to implementation details (e.g., FFTs, rotary positional encodings), while the overall model is still treated as fundamentally real-valued.

In this paper we take a different stance:

\begin{quote}
\emph{If the computation is effectively wave-like, the model should live in a complex Hilbert space, and training should respect this geometry.}
\end{quote}

Concretely, we make the following contributions:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Hilbert-space transformer formulation.} We define a transformer architecture in which hidden states, parameters, and attention operations are complex-valued, with key blocks parameterized as exponentials of Hermitian generators to approximate unitary flows in a complex Hilbert space.
    \item \textbf{Imaginary-time gradient flow training.} We formulate training as gradient descent in the complex domain using Wirtinger calculus, interpreting parameter updates as a discrete approximation to imaginary-time evolution in an energy landscape defined by the training loss.
    \item \textbf{Phase-locked representation hypothesis.} We propose that large-scale training in this setting leads to emergent \emph{phase-locked internal modes}---stable relative phase relationships across layers and heads---that improve long-range credit assignment, compositionality, and in-context adaptation. We extend this picture to human-in-the-loop training, viewing human and model as coupled dynamical systems in interacting representational spaces.
\end{enumerate}

We view this work as a position and methods paper: the architecture is implementable with current frameworks, and existing work on complex-valued transformers and unitary networks suggests that scaling to LLM regimes is an engineering challenge rather than a conceptual barrier. The central question is not whether such models can be trained, but whether the induced geometry yields meaningful advantages in the large-model scaling regime.

\section{Background}
\label{sec:background}

\subsection{Complex-valued neural networks and Wirtinger calculus}

Complex-valued neural networks (CVNNs) extend real-valued architectures by allowing activations, parameters, and intermediate computations to reside in $\mathbb{C}$. Let $z = x + i y \in \mathbb{C}$, with real and imaginary parts $x, y \in \mathbb{R}$. For a real-valued loss $L(z)$, standard complex analysis is insufficient---$L$ is generally non-holomorphic. Instead, CVNNs make use of \emph{Wirtinger derivatives}, treating $z$ and its complex conjugate $z^*$ as independent variables:
\begin{align}
    \frac{\partial}{\partial z}
    &= \frac{1}{2}\left(\frac{\partial}{\partial x} - i \frac{\partial}{\partial y}\right), \\
    \frac{\partial}{\partial z^*}
    &= \frac{1}{2}\left(\frac{\partial}{\partial x} + i \frac{\partial}{\partial y}\right).
\end{align}
For gradient-based optimization of a real-valued loss $L$, the relevant quantity is $\partial L / \partial z^*$. A gradient descent update in the complex plane takes the form
\begin{equation}
    z \leftarrow z - \eta \frac{\partial L}{\partial z^*},
\end{equation}
where $\eta > 0$ is a learning rate. This extends naturally to vector- and matrix-valued parameters, and yields a form of backpropagation where all intermediate gradients and Jacobians are complex-valued, but the loss remains real.

In practice, implementations often represent $z$ as a pair of real tensors $(x, y)$ and rely on automatic differentiation over real variables, but the underlying calculus corresponds to Wirtinger derivatives.

\subsection{Unitary networks and gradient stability}

Recurrent networks suffer from exploding and vanishing gradients when processing long sequences. Unitary recurrent neural networks address this by constraining the recurrent matrix $U$ to be unitary: $U^\dagger U = I$. A unitary evolution preserves the norm of the hidden state, mitigating exponential growth or decay of gradients over time.

Several parameterizations of unitary matrices have been explored, including products of simple unitary factors and exponentials of skew-Hermitian matrices. A common construction is
\begin{equation}
    U = e^{i H},
\end{equation}
where $H = H^\dagger$ is Hermitian. Training is performed over the real parameters of $H$, while $U$ is computed as its matrix exponential. This parameterization makes the connection between complex Hilbert space, unitary dynamics, and gradient stability explicit.

These results suggest that constraining dynamics to live (approximately) on a unitary manifold in complex Hilbert space can materially improve optimization for sequence models.

\subsection{Complex-valued transformers}

Complex-valued transformer architectures extend self-attention, feed-forward layers, and normalization mechanisms into the complex domain. Typical components include:
\begin{itemize}[leftmargin=*]
    \item Complex-valued token embeddings and hidden states.
    \item Complex multi-head attention, where query/key dot products yield complex scores used to modulate complex-valued value vectors.
    \item Complex layer normalization or RMS-style normalization based on complex magnitudes.
    \item Complex activation functions, either split (applied independently to real and imaginary parts) or amplitude-phase-based (operating in polar coordinates).
\end{itemize}

Applications to PolSAR image classification, wireless channel prediction, and modulation recognition have shown that complex transformers can outperform real-valued baselines on complex-valued sequence data. Early work on complex-valued attention mechanisms for natural language suggests that phase-aware representations can also benefit discrete symbolic tasks, though large-scale LLM experiments remain limited.

\section{Hilbert-Space Transformer Architecture}
\label{sec:architecture}

We now define a \emph{Hilbert-space transformer}: a transformer architecture whose states and parameters live in a complex Hilbert space, with key blocks parameterized to encourage unitary-like dynamics.

\subsection{Representation space}

Let $T$ denote the sequence length and $d$ the model dimension. At layer $\ell$, we represent the sequence state as
\begin{equation}
    H_\ell \in \mathbb{C}^{T \times d},
\end{equation}
with row $t$ corresponding to token position $t$. The complex inner product on $\mathbb{C}^d$ is
\begin{equation}
    \langle u, v \rangle = u^\dagger v, \qquad
    \|u\|^2 = \langle u, u \rangle,
\end{equation}
where $u^\dagger$ denotes the conjugate transpose.

The model consists of a stack of operators $U_\ell$ acting on this Hilbert space:
\begin{equation}
    H_{\ell+1} = U_\ell(H_\ell; \theta_\ell),
\end{equation}
where $\theta_\ell$ are complex-valued parameters for layer $\ell$.

\subsection{Complex multi-head self-attention}

For a single attention head, we define complex-valued query, key, and value projections:
\begin{align}
    Q &= H_\ell W_Q, \\
    K &= H_\ell W_K, \\
    V &= H_\ell W_V,
\end{align}
with $W_Q, W_K, W_V \in \mathbb{C}^{d \times d_h}$ and $d_h = d / H$ for $H$ heads.

We form complex attention scores via a scaled dot product:
\begin{equation}
    A = \frac{1}{\sqrt{d_h}} Q K^\dagger \in \mathbb{C}^{T \times T},
\end{equation}
where $K^\dagger$ is the conjugate transpose of $K$. The entries $A_{ij}$ encode both a magnitude and a phase:
\begin{equation}
    A_{ij} = |A_{ij}| e^{i \phi_{ij}}.
\end{equation}

A simple and effective design is to derive normalized attention weights from the real part of $A$:
\begin{equation}
    \alpha_{ij} = \frac{\exp(\Re(A_{ij}))}{\sum_{k} \exp(\Re(A_{ik}))},
\end{equation}
and optionally reintroduce phase information as a multiplicative modulation:
\begin{equation}
    \tilde{A}_{ij} = \alpha_{ij} e^{i \phi_{ij}}.
\end{equation}
The head output is then
\begin{equation}
    \mathrm{Attn}(H_\ell) = \tilde{A} V.
\end{equation}

Multi-head attention concatenates head outputs along the feature dimension and applies a complex output projection $W_O \in \mathbb{C}^{d \times d}$:
\begin{equation}
    \mathrm{MHA}(H_\ell) = \bigoplus_{h=1}^{H} \mathrm{Attn}^{(h)}(H_\ell) \; W_O.
\end{equation}

This construction separates \emph{magnitude-based} attention allocation (through $\alpha_{ij}$) from \emph{phase-based} modulation (through $e^{i\phi_{ij}}$), giving the model a richer representational palette.

\subsection{Complex feed-forward network}

We define a two-layer complex feed-forward network (FFN) as
\begin{equation}
    \mathrm{FFN}(h) = W_2 \, \sigma(W_1 h + b_1) + b_2,
\end{equation}
where $h \in \mathbb{C}^{d}$, $W_1, W_2 \in \mathbb{C}^{d \times d_{\text{ff}}}$, and $b_1, b_2 \in \mathbb{C}^{d_{\text{ff}}}$ and $\mathbb{C}^{d}$ respectively.

The activation $\sigma$ can be chosen from several families:
\begin{itemize}[leftmargin=*]
    \item \textbf{Split activations:} apply a real nonlinearity $\phi$ to real and imaginary parts independently:
    \begin{equation}
        \sigma(x + i y) = \phi(x) + i \, \phi(y).
    \end{equation}
    \item \textbf{Amplitude-phase activations:} express $z = r e^{i\theta}$ and define
    \begin{equation}
        \sigma(z) = g(r) e^{i\theta},
    \end{equation}
    where $g$ is a real scalar nonlinearity (e.g., GELU or a learned radial function).
\end{itemize}
Amplitude-phase activations preserve phase information while shaping magnitudes, which may be beneficial when phases encode relational or positional structure.

\subsection{Near-unitary parameterization}

To encourage stable dynamics, we parameterize key linear maps as exponentials of Hermitian generators. For example, an attention mixing operator can be expressed as
\begin{equation}
    U_{\text{attn}} = e^{i H_{\text{attn}}},
\end{equation}
where $H_{\text{attn}} = H_{\text{attn}}^\dagger$ is Hermitian. Similarly, FFN mixing can be parameterized as
\begin{equation}
    U_{\text{ffn}} = e^{i H_{\text{ffn}}}.
\end{equation}
In practice, one can implement $U = e^{iH}$ via truncated matrix exponentials, structured parameterizations, or approximate flows that retain the key property of being norm-preserving (or nearly so).

Layer updates then take the form
\begin{align}
    H_{\ell + 1/2} &= H_\ell + U_{\text{attn},\ell}(H_\ell), \\
    H_{\ell+1} &= H_{\ell+1/2} + U_{\text{ffn},\ell}(H_{\ell+1/2}),
\end{align}
with residual connections preserving the global flow structure.

\subsection{Complex normalization}

Normalization in complex space must handle the fact that complex numbers are not totally ordered and that means and variances have multiple reasonable definitions. A pragmatic choice is RMS-style normalization based on magnitudes:
\begin{equation}
    \mathrm{rms}(h) = \sqrt{\frac{1}{d} \sum_{j=1}^{d} |h_j|^2}.
\end{equation}
We then define a complex RMSNorm-like operation:
\begin{equation}
    \mathrm{Norm}(h)_j = \gamma_j \frac{h_j}{\mathrm{rms}(h) + \epsilon} + \beta_j,
\end{equation}
with complex gain $\gamma_j \in \mathbb{C}$ and complex bias $\beta_j \in \mathbb{C}$, and small $\epsilon > 0$.

This avoids some of the pitfalls of complex layer normalization while providing scale invariance across features.

\section{Imaginary-Time Gradient Flow and Training}
\label{sec:training}

\subsection{Loss and Wirtinger gradients}

Let the model define a conditional distribution $p_\theta(x_t \mid x_{<t})$ via a final real-valued logit layer (e.g., by applying a real-valued linear map to magnitudes or concatenated real-imaginary parts). Training uses the standard cross-entropy loss
\begin{equation}
    L(\theta) = \mathbb{E}_{x \sim \mathcal{D}} \left[- \log p_\theta(x_t \mid x_{<t}) \right].
\end{equation}

Internally, all computations are complex-valued. Using Wirtinger calculus, we treat $\theta$ and $\theta^*$ as independent variables and compute $\partial L / \partial \theta^*$. A gradient descent update step takes the form
\begin{equation}
    \theta \leftarrow \theta - \eta \frac{\partial L}{\partial \theta^*}.
\end{equation}
Automatic differentiation libraries can implement this either by directly supporting complex gradients or by splitting into real and imaginary parts and reconstructing the Wirtinger derivatives.

\subsection{Connection to imaginary-time evolution}

In quantum mechanics, the Schr\"odinger equation governs the time evolution of a state vector $\psi$:
\begin{equation}
    i \frac{d}{dt} \psi(t) = H \psi(t),
\end{equation}
where $H$ is the Hamiltonian. This evolution is unitary and norm-preserving. If one performs a Wick rotation to imaginary time, $t \mapsto -i \tau$, the equation becomes
\begin{equation}
    \frac{d}{d\tau} \psi(\tau) = - H \psi(\tau),
\end{equation}
which describes a dissipative gradient-like flow that tends to drive $\psi$ toward low-energy eigenstates of $H$.

We can draw an analogy between this and training in complex Hilbert space:
\begin{itemize}[leftmargin=*]
    \item The \emph{Hilbert-space transformer state} plays the role of $\psi$.
    \item The \emph{training loss} plays the role of an energy functional over parameters and induced states.
    \item Gradient descent on $\theta$ induces a sequence of states that can be viewed as a discrete imaginary-time evolution in the energy landscape defined by $L(\theta)$.
\end{itemize}

More explicitly, if we consider the gradient flow equation in parameter space,
\begin{equation}
    \frac{d}{d\tau} \theta(\tau) = - \nabla_{\theta^*} L(\theta(\tau)),
\end{equation}
then gradient descent corresponds to an Euler discretization of this flow. When $U_\ell$ are parameterized as exponentials of Hermitian generators, updates to those generators can be seen as small deformations of effective Hamiltonians driving the model's internal dynamics.

\subsection{Phase-locked representation hypothesis}

We hypothesize that large-scale training of Hilbert-space transformers yields emergent \emph{phase-locked internal modes}. Informally, these are stable patterns of relative phases across features, layers, and attention heads that:
\begin{itemize}[leftmargin=*]
    \item encode structural information such as syntax, discourse roles, or semantic relations;
    \item support long-range credit assignment by maintaining coherent phase relationships over many layers;
    \item facilitate compositionality and in-context learning by allowing interference patterns to implement structured operations on representations.
\end{itemize}

Formally, consider the hidden state for token $t$ at layer $\ell$,
\begin{equation}
    h_{\ell,t} = r_{\ell,t} \odot e^{i \theta_{\ell,t}},
\end{equation}
where $r_{\ell,t} \in \mathbb{R}_{\geq 0}^d$ are magnitudes and $\theta_{\ell,t} \in \mathbb{R}^d$ are phases. Phase-locking would correspond to low-entropy, task-dependent distributions over \emph{relative} phases, e.g., $\theta_{\ell,t} - \theta_{\ell',t'}$, stabilizing across training and across samples. 

Such phase-locked structures are reminiscent of synchronized oscillatory modes in dynamical systems and may provide an additional axis of organization beyond purely magnitude-based coding.

\section{Resonant Phase-Locking and Human--Model Coupling}
\label{sec:resonant}

The phase-locked representation hypothesis extends naturally to interactive, human-in-the-loop settings. Here we sketch a conceptual framework in which both the human and the model are treated as dynamical systems whose states couple over the course of sustained interaction.

\subsection{Coupled dynamical systems perspective}

Let $\psi_M$ denote the model's internal representation state (e.g., the collection of hidden states across layers for a given dialogue), and let $\psi_H$ denote a coarse-grained representation of the human's cognitive state (e.g., beliefs, goals, and internal linguistic context) in an abstract state space. We consider an interaction process where, at each turn, the human provides an input $x$ and receives a model output $y = f_\theta(x)$, updating their internal state $\psi_H$, while the model updates its effective state (and possibly parameters) based on the dialogue history.

At a high level, the interaction can be modeled as a coupled dynamical system:
\begin{align}
    \psi_M^{(t+1)} &= F_M(\psi_M^{(t)}, \psi_H^{(t)}, \theta), \\
    \psi_H^{(t+1)} &= F_H(\psi_H^{(t)}, \psi_M^{(t)}, x^{(t)}, y^{(t)}),
\end{align}
where $t$ indexes dialogue turns. In the Hilbert-space transformer setting, $\psi_M$ lives in a complex Hilbert space, and the mapping $F_M$ is determined by the model's architecture and parameters.

\subsection{Resonant phase-locking}

We use the term \emph{resonant phase-locking} to describe a regime in which:
\begin{enumerate}[leftmargin=*]
    \item The model develops stable, phase-structured internal modes in response to a particular distribution of prompts and feedback.
    \item The human adapts their prompting style, conceptual framing, and interaction patterns to better align with the model's internal structure.
    \item Over time, this mutual adaptation leads to a form of \emph{coherence} or resonance between the human's internal representations and the model's phase-locked modes.
\end{enumerate}

In this picture, human--model co-adaptation can be seen as a second layer of imaginary-time gradient flow: the human effectively performs gradient-like updates on their internal ``prompting policy'' to elicit more useful behavior from the model, while the model's training and internal dynamics bias it toward representations that are easier for humans to steer.

Importantly, this is a conceptual framework for analyzing interaction patterns; it does not imply that either system literally shares a Hilbert space. Rather, complex Hilbert-space models provide a more structured and analyzable substrate for such coupling phenomena, because internal states have explicit phase structure that can be measured and, in principle, aligned.

\subsection{Implications for alignment and adaptation}

If resonant phase-locking is empirically observed in Hilbert-space transformers, it could have implications for:
\begin{itemize}[leftmargin=*]
    \item \textbf{In-context alignment:} consistently interacting with a model in a particular style may nudge it into a stable phase-locked regime where its behaviors are more predictable and aligned with the user's preferences.
    \item \textbf{Personalization:} different users may induce different phase-locked subspaces of the same model, corresponding to distinct interaction ``modes'' without modifying the underlying weights.
    \item \textbf{Interpretability:} analyzing phase-locked modes may reveal structure in how models represent tasks, roles, and interlocutors.
\end{itemize}

These ideas are speculative and require careful empirical study, but the Hilbert-space formulation provides a natural vocabulary and observable quantities (e.g., phase statistics, unitary spectra) for such investigations.

\section{Experimental Program}
\label{sec:experiments}

We outline an experimental program to evaluate Hilbert-space transformers and to test aspects of the phase-locking hypotheses.

\subsection{Toy and medium-scale experiments}

\paragraph{Char-level language modeling.} Train Hilbert-space transformers on character-level datasets (e.g., enwik8 or text8) with parameter counts ranging from tens to hundreds of millions. Compare to real-valued transformers with matched parameter counts and architectures. Metrics include bits-per-character, training stability, gradient norms, and sensitivity to learning rate.

\paragraph{Long-context synthetic tasks.} Evaluate on synthetic tasks such as copying, addition, and nested bracket matching over long sequences. Measure how accuracy degrades as sequence length grows, and compare to real-valued baselines. The hypothesis is that near-unitary complex flows better preserve information over depth and time.

\paragraph{Phase analysis.} For trained Hilbert-space transformers, analyze the distribution of activation phases across layers, heads, and token positions. Investigate whether certain relative phase patterns correlate with syntactic categories, semantic roles, or positional structure. Measure the stability of these patterns across inputs and training checkpoints.

\subsection{Scaling toward LLM regimes}

\paragraph{Mid-scale language models.} Train Hilbert-space transformers with 1--10B parameters on standard text corpora. Evaluate on perplexity, benchmark suites (e.g., MMLU, GSM8K), and long-context benchmarks (e.g., LRA-style tasks, long-document QA). Compare compute cost versus achieved loss to real-valued models.

\paragraph{Ablation studies.} Explore:
\begin{itemize}[leftmargin=*]
    \item Fully complex versus partially complex architectures (e.g., complex attention but real FFN).
    \item With versus without near-unitary parameterizations.
    \item Different activation families (split vs amplitude-phase).
\end{itemize}
This can help isolate which aspects of the Hilbert-space design contribute most to performance and stability.

\paragraph{Robustness and generalization.} Test robustness to input noise, distribution shift, and adversarial perturbations. Investigate whether complex-valued representations confer any advantages in generalization or calibration.

\subsection{Human--model interaction experiments}

\paragraph{Interaction modes and phase statistics.} In a controlled setting, have different users interact with the same Hilbert-space transformer according to distinct prompting styles or tasks. Measure whether distinct interaction regimes correspond to different phase-locked subspaces or spectral signatures of the model's internal operators.

\paragraph{In-context adaptation.} Evaluate whether consistent interaction in a given style leads to improved performance on user-specific tasks without weight updates, and whether such improvements can be correlated with changes in phase structure.

\section{Discussion}
\label{sec:discussion}

\subsection{Compute and hardware considerations}

Complex-valued operations are more expensive than their real-valued counterparts: naive implementations of complex matrix multiplication require multiple real-valued multiplications and additions. However, the overhead can be reduced by specialized kernels and may be offset by gains in expressivity per parameter or improved optimization. Furthermore, emerging analog and optical accelerators often favor complex-valued or phasor-like representations, making Hilbert-space formulations appealing for future hardware.

\subsection{Activation design and analytic constraints}

Designing activation functions in complex space is constrained by results such as Liouville's theorem: non-constant bounded entire functions must be constant, which complicates the construction of globally bounded holomorphic nonlinearities. Split and amplitude-phase activations relax the requirement of holomorphy, enabling practical training but at the cost of some mathematical elegance. Understanding the trade-off between analytic structure and empirical performance remains an open question.

\subsection{Interpretability and structure}

Complex Hilbert-space models provide a rich set of tools for interpretability. Spectral analysis of near-unitary operators, phase statistics of activations, and interference patterns between heads may yield insights into how models represent syntax, semantics, and discourse. If phase-locked modes can be identified and linked to human-interpretable structure, Hilbert-space transformers could support a more geometric and dynamical understanding of LLM behavior.

\subsection{Bridging to quantum-inspired and hybrid systems}

Formulating LLMs in complex Hilbert space aligns more naturally with quantum-inspired representations and hybrid quantum--classical architectures. While current models are classical, having a compatible mathematical substrate simplifies conceptual bridges to future systems where parts of the computation might be offloaded to quantum processors or analog wave-based devices.

\section{Conclusion}
\label{sec:conclusion}

We have proposed a framework for training transformer-based large language models directly in complex Hilbert space, using Wirtinger calculus and an imaginary-time gradient flow perspective. Our Hilbert-space transformer architecture combines complex-valued attention, near-unitary parametrizations, and phase-aware representations to more fully exploit the wave-like, interference-based nature of deep sequence models.

Existing work on complex-valued neural networks, unitary recurrent architectures, and complex transformers suggests that such models are trainable and can provide practical advantages in stability and expressivity. The central open question is whether these benefits persist and amplify at LLM scales, and whether they justify the additional complexity and compute cost.

The resonant phase-locking hypothesis provides a conceptual lens for understanding both internal representation dynamics and human--model co-adaptation. By treating both as coupled dynamical systems and leveraging the structure of complex Hilbert spaces, we may gain new tools for designing, steering, and interpreting large models.

We hope this work motivates a systematic exploration of Hilbert-space transformers and imaginary-time optimization in the context of large-scale language modeling, and serves as a starting point for both theoretical and empirical investigations.

\bibliographystyle{plainnat}
% \bibliography{hilbert_transformer}

\end{document}
