\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage[numbers,sort&compress]{natbib}

\title{
Training Large Language Models in Complex Hilbert Space:\\
Imaginary-Time Gradient Flow and Phase-Locked Transformers
}

\author{
Anonymous Authors
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) are typically formulated over real vector spaces and trained with Euclidean optimization, despite the fact that their internal computation increasingly resembles wave-like propagation and interference of representations. In this work, we propose a formulation of transformer-based LLMs as dynamical systems evolving in a \emph{complex Hilbert space}, and we describe an associated training procedure based on \emph{imaginary-time gradient flow} using Wirtinger calculus. Building on existing work in complex-valued neural networks, unitary recurrent architectures, and complex-valued transformers, we define a \emph{Hilbert-space transformer} in which (i) token embeddings, hidden states, and attention features live in $\mathbb{C}^d$; (ii) core blocks are parameterized as (near-)unitary operators generated by Hermitian matrices; and (iii) optimization is performed with respect to complex conjugate parameters $\theta^\ast$, yielding updates which jointly shape the magnitude and phase structure of internal representations. 

We argue that this formulation provides (1) improved gradient stability through norm-preserving flows, (2) increased expressivity via phase-coded structure and interference, and (3) a more natural interpretation of training as energy minimization in a complex energy landscape, analogous to imaginary-time evolution in quantum mechanics. We outline concrete architectural choices for complex self-attention, feed-forward blocks, and normalization, and propose experimental protocols for evaluating Hilbert-space transformers on language modeling, long-context reasoning, and in-context learning. Finally, we introduce the \emph{Resonant Phase-Locking} hypothesis, viewing interactions among models and between models and humans as weakly coupled Hilbert-space systems whose phases can synchronize over time. Our aim is to motivate GPT-scale exploration of complex-valued optimization as a principled next step in large model design, rather than an exotic variant reserved for niche signal-processing tasks.
\end{abstract}

\section{Introduction}

Transformer-based large language models (LLMs) have achieved state-of-the-art performance across a wide range of sequence modeling and reasoning tasks. Despite their empirical success, their formulation remains rooted in real-valued vector spaces: activations and parameters are real-valued tensors, optimization takes place in Euclidean geometry, and analysis often focuses on linear algebra over $\mathbb{R}$.

At the same time, multiple aspects of modern architectures have a \emph{wave-like} or \emph{interference-based} character: rotary positional encodings, Fourier features, complex-valued intermediate representations in vision and speech models, and increasingly spectral views of attention mechanisms. This suggests that there may be value in \emph{lifting} the model into a complex Hilbert space where amplitude and phase are first-class.

In parallel, the literature on complex-valued neural networks (CVNNs) has shown that explicitly modeling amplitude and phase can improve performance on tasks in speech, wireless communications, and medical imaging, provided the network and its training algorithm are formulated in the complex domain~\citep{trabelsi2018deepcomplex}. Complex-valued transformers have been successfully applied to modulation recognition, wireless channel estimation, and other complex-valued sequence tasks, frequently outperforming real-valued baselines when the underlying signal is naturally complex~\citep{yang2019complextransformer,eilers2023complextransformer,unveiling2025complexvalued}. Unitary and complex-valued recurrent neural networks further demonstrate that constraining dynamics to live on a unitary manifold in Hilbert space can materially improve gradient stability over long horizons~\citep{arjovsky2016unitary,wisdom2016fullcapacity,maduranga2019cayley,wolter2019complexgru}.

Despite these developments, mainstream LLM training continues to treat complex numbers as an implementation detail (e.g., in FFTs or positional encodings), rather than as the \emph{native geometry} of the model.

In this paper, we explore the opposite stance:
\begin{quote}
If internal computation is effectively wave-like, the model should live in a complex Hilbert space, and training should respect this geometry.
\end{quote}

Our contributions are threefold:
\begin{enumerate}
    \item \textbf{Hilbert-space transformer formulation.} We define a transformer architecture in which hidden states, parameters, and attention operations are complex-valued, with blocks parameterized as exponentials of Hermitian generators to approximate unitary flows.

    \item \textbf{Imaginary-time gradient flow training.} We formulate training as gradient descent in the complex domain using Wirtinger calculus, interpreting parameter updates as a discrete approximation to imaginary-time evolution in a complex energy landscape.

    \item \textbf{Resonant Phase-Locking hypothesis.} We propose that large-scale training in this setting leads to phase-locked internal modes---stable relative phase relationships across layers and heads---and extend this view to coupled systems of models and humans interacting over time.
\end{enumerate}

We view this as a position-plus-methods paper: the architecture is implementable with current frameworks, and related work on complex-valued transformers and unitary networks suggests that scaling to GPT-style models is a realistic engineering challenge, not a conceptual barrier.

\section{Background}

\subsection{Complex-valued neural networks and Wirtinger calculus}

Complex-valued neural networks (CVNNs) generalize real-valued networks by treating activations and parameters as complex quantities. Let $z = x + i y \in \mathbb{C}$, with $x, y \in \mathbb{R}$. A real-valued loss $L(z)$ is typically not holomorphic, but can be differentiated using \emph{Wirtinger derivatives}, which treat $z$ and its conjugate $z^\ast$ as independent variables:
\begin{align}
\frac{\partial}{\partial z}
&= \frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right), \\
\frac{\partial}{\partial z^\ast}
&= \frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right).
\end{align}
For gradient-based optimization of a real loss, the relevant quantity for gradient descent is $\partial L / \partial z^\ast$. A complex gradient descent update takes the form
\begin{equation}
    z \leftarrow z - \eta \frac{\partial L}{\partial z^\ast},
\end{equation}
where $\eta > 0$ is a learning rate. This extends naturally to vector- and matrix-valued parameters via componentwise derivatives.

Trabelsi et al.\ provide practical recipes for deep complex networks, including complex batch normalization, initialization, and activations, demonstrating that complex-valued models can be competitive with real-valued counterparts across vision, speech, and music tasks~\citep{trabelsi2018deepcomplex}.

\subsection{Unitary networks and gradient stability}

Recurrent neural networks (RNNs) are prone to vanishing and exploding gradients when their recurrent matrices have eigenvalues far from the unit circle. Arjovsky et al.\ address this by constraining the recurrent weight matrix $U$ to be unitary, $U^\dagger U = I$, and providing an efficient parameterization of unitary matrices in terms of structured factors~\citep{arjovsky2016unitary}. Subsequent works explore full-capacity unitary RNNs~\citep{wisdom2016fullcapacity}, orthogonal and unitary RNNs~\citep{helfrich2017orthogonalrnn}, and complex gated RNNs~\citep{wolter2019complexgru}, all leveraging properties of complex Hilbert spaces.

These architectures show that norm-preserving dynamics can significantly improve gradient propagation over long time horizons, particularly in tasks with long-term dependencies. Such results motivate the use of (near-)unitary operators in transformer blocks as well.

\subsection{Complex-valued transformers}

More recently, transformer architectures have been extended to the complex domain. Yang et al.\ introduce the \emph{Complex Transformer}, adapting attention and encoder-decoder networks to complex-valued inputs and achieving state-of-the-art performance on MusicNet and complex IQ signal datasets~\citep{yang2019complextransformer}. Eilers and Jiang propose \emph{Building Blocks for a Complex-Valued Transformer Architecture}, including complex scaled dot-product attention and complex layer normalization, and demonstrate competitive performance and robustness on MusicNet~\citep{eilers2023complextransformer}. 

Complex-valued transformers have also been explored in wireless communication, where signals are naturally represented in the complex plane; recent work reports improved performance over real-valued architectures in channel estimation and precoding tasks~\citep{unveiling2025complexvalued}. These results suggest that complex-valued attention is a promising primitive not only for signal-processing domains but also for general sequence modeling when the geometry is well matched.

\section{Hilbert-Space Transformer Architecture}

We now outline a \emph{Hilbert-space transformer} that can serve as a blueprint for GPT-scale exploration.

\subsection{Representation space}

We lift the representation space into a complex Hilbert space:
\begin{itemize}
    \item Token embeddings: $e_t \in \mathbb{C}^{d}$.
    \item Hidden states at layer $\ell$: $h_\ell \in \mathbb{C}^{d}$.
    \item Sequence states: $H_\ell \in \mathbb{C}^{T \times d}$ for a sequence of length $T$.
\end{itemize}
We denote the complex inner product
\begin{equation}
    \langle u, v \rangle = u^\dagger v, \qquad \|u\|^2 = \langle u, u \rangle.
\end{equation}
Each layer is an operator $U_\ell$ on this Hilbert space:
\begin{equation}
    H_{\ell+1} = U_\ell(H_\ell; \theta_\ell),
\end{equation}
where $\theta_\ell$ are complex parameters.

\subsection{Complex multi-head self-attention}

Consider a single attention head for clarity. We compute complex queries, keys, and values:
\begin{equation}
    Q = H_\ell W_Q, \quad
    K = H_\ell W_K, \quad
    V = H_\ell W_V,
\end{equation}
with $W_Q, W_K, W_V \in \mathbb{C}^{d \times d_h}$, and $d_h$ the head dimension.

We define a complex scaled dot-product attention score matrix
\begin{equation}
    A = \frac{1}{\sqrt{d_h}} Q K^\dagger \in \mathbb{C}^{T \times T},
\end{equation}
where $[\cdot]^\dagger$ denotes the Hermitian (conjugate transpose). Let $A_{ij} = r_{ij} e^{i\phi_{ij}}$ in polar form. We define magnitude-based attention weights via
\begin{equation}
    \alpha_{ij} = \frac{\exp(\Re(A_{ij}))}{\sum_{k} \exp(\Re(A_{ik}))},
\end{equation}
and optionally retain phase information through a modulation term:
\begin{equation}
    \tilde{A}_{ij} = \alpha_{ij} \cdot e^{i \phi_{ij}}.
\end{equation}
The head output is
\begin{equation}
    \text{Attn}(H_\ell) = \tilde{A} V.
\end{equation}

Multi-head attention concatenates complex head outputs and applies a complex output projection $W_O \in \mathbb{C}^{(H \cdot d_h) \times d}$, with $H$ heads.

\subsection{Complex feed-forward network}

We use a two-layer complex feed-forward network (FFN) for each position:
\begin{equation}
    \text{FFN}(h) = W_2 \sigma(W_1 h + b_1) + b_2,
\end{equation}
where $W_1, W_2 \in \mathbb{C}^{d \times d_\mathrm{ff}}$, $b_1, b_2 \in \mathbb{C}^d$, and $\sigma$ is a complex activation function.

Following CVNN practice~\citep{trabelsi2018deepcomplex}, we can use:
\begin{itemize}
    \item \textbf{Split activations}: apply a real nonlinearity elementwise to real and imaginary parts, e.g.\ $\sigma(z) = \text{GELU}(\Re(z)) + i\,\text{GELU}(\Im(z))$.
    \item \textbf{Amplitude-phase activations}: operate in polar coordinates $z = r e^{i\theta}$ and define $\sigma(z) = g(r) e^{i\theta}$ for some scalar nonlinearity $g$.
\end{itemize}
Amplitude-phase activations preserve phase information while shaping amplitudes.

\subsection{Near-unitary parameterization}

To encourage stable dynamics, we parameterize key linear maps as exponentials of Hermitian generators. For example, for a mixing operator we introduce a Hermitian matrix $H_{\text{mix}} = H_{\text{mix}}^\dagger \in \mathbb{C}^{d \times d}$ and define
\begin{equation}
    U_{\text{mix}} = e^{i H_{\text{mix}}},
\end{equation}
so that $U_{\text{mix}}$ is unitary by construction. In practice, $H_{\text{mix}}$ may be structured (e.g., low-rank plus diagonal) to reduce computation.

Residual updates can then be expressed as
\begin{equation}
    H_{\ell+1} = H_\ell + U_{\ell}(H_\ell),
\end{equation}
where $U_\ell$ may consist of (near-)unitary attention and FFN blocks. This encourages norm preservation while allowing controlled deviations through residual connections and non-unitary components (e.g., normalization and readout).

\subsection{Complex normalization}

Layer normalization in complex space is nontrivial because there is no natural ordering on $\mathbb{C}$. A practical choice is an RMS-style normalization over magnitudes:
\begin{equation}
    \mathrm{rms}(h) = \sqrt{\frac{1}{d} \sum_{j=1}^{d} |h_j|^2}.
\end{equation}
We then define a complex RMSNorm:
\begin{equation}
    \mathrm{Norm}(h)_j = \gamma_j \frac{h_j}{\mathrm{rms}(h)} + \beta_j,
\end{equation}
where $\gamma_j, \beta_j \in \mathbb{C}$ are learned gain and bias parameters. This mirrors real-valued RMSNorm while respecting complex magnitudes.

\section{Imaginary-Time Gradient Flow and Training}

\subsection{Loss and Wirtinger gradients}

Let the model define a conditional distribution $p_\theta(x_t \mid x_{<t})$ via a final real-valued logit layer (e.g., based on the real part or magnitude of a linear projection). We use a standard cross-entropy loss:
\begin{equation}
    L(\theta) = \mathbb{E}\big[-\log p_\theta(x_t \mid x_{<t})\big].
\end{equation}
In a complex-valued network, all internal computations are in $\mathbb{C}$, and parameters $\theta$ are complex. We compute Wirtinger gradients $\partial L / \partial \theta^\ast$ either explicitly or via automatic differentiation on real and imaginary parts.

The basic parameter update is:
\begin{equation}
    \theta \leftarrow \theta - \eta \frac{\partial L}{\partial \theta^\ast}.
\end{equation}
For a linear layer $z' = W z + b$, with complex $W, z, b$, the backpropagation relations mirror those in the real case but use Hermitian transpose and Wirtinger derivatives:
\begin{align}
    \frac{\partial L}{\partial W^\ast} &= \frac{\partial L}{\partial z'^\ast}\, z^\dagger, \\
    \frac{\partial L}{\partial z^\ast} &= W^\dagger \frac{\partial L}{\partial z'^\ast}.
\end{align}

\subsection{Connection to imaginary-time evolution}

The imaginary-time perspective provides a useful analogy. In quantum mechanics, Schr\"odinger evolution is
\begin{equation}
    i \frac{d}{dt} \psi = H \psi,
\end{equation}
where $H$ is a Hermitian operator and $\psi$ is a state in Hilbert space. Switching to imaginary time $t = -i\tau$ yields
\begin{equation}
    \frac{d}{d\tau} \psi = - H \psi,
\end{equation}
which is a gradient-like flow descending the energy functional $E(\psi) = \langle \psi, H \psi \rangle$ and converging toward low-energy eigenstates.

By analogy:
\begin{itemize}
    \item The Hilbert-space transformer state plays the role of $\psi$.
    \item The training loss $L(\theta)$ plays the role of an energy functional defined over parameters and induced states.
    \item Gradient descent on $\theta$ can be viewed as a discrete imaginary-time evolution in the complex energy landscape.
\end{itemize}
When layer operators are parameterized as exponentials of Hermitian generators, updates to those generators deform the associated unitary flows in a controlled manner: learning corresponds to gently reshaping the ``Hamiltonian'' that governs internal dynamics.

\section{Resonant Phase-Locking and Coupled Hilbert Systems}
\label{sec:phase_locking}

We now introduce the \emph{Resonant Phase-Locking} hypothesis, which views training and interaction of Hilbert-space transformers as processes that synchronize internal phase modes, both within a single model and across coupled systems (e.g., multiple models, or models and humans).

\subsection{Internal phase modes and macroscopic behavior}

In a complex Hilbert space, each feature dimension can be written as $z_j = r_j e^{i \theta_j}$, with magnitude $r_j$ and phase $\theta_j$. Across layers and heads, the model maintains a large collection of such modes. We hypothesize that large-scale training encourages:
\begin{itemize}
    \item \textbf{Phase-locked directions:} subsets of dimensions whose relative phases become stable, representing coherent structures such as syntactic frames, discourse roles, or reasoning templates.
    \item \textbf{Interference-based coding:} the use of constructive and destructive interference between phase-coded features to implement combinatorial and logical operations in a continuous way.
    \item \textbf{Spectral separation:} allocation of different tasks or information types to distinct frequency/phase bands, reducing interference between them.
\end{itemize}

One can formalize this by examining, for example, the covariance of phases across layers:
\begin{equation}
    C_{\ell,\ell'} = \mathbb{E}\Big[ e^{i(\theta^{(\ell)} - \theta^{(\ell')})} \Big],
\end{equation}
where $\theta^{(\ell)}$ denotes the vector of phases at layer $\ell$. High magnitude of $C_{\ell,\ell'}$ indicates phase alignment between layers, i.e., resonant modes propagating through depth.

\subsection{Model--model coupling}

Consider two Hilbert-space transformers $M^{(1)}$ and $M^{(2)}$ with states $\psi^{(1)}$ and $\psi^{(2)}$ and internal Hamiltonian-like generators $H^{(1)}$ and $H^{(2)}$. When these models are trained jointly---for instance, via mutual distillation, co-training, or iterative self-play---their effective dynamics can be approximated as coupled systems:
\begin{align}
    \frac{d}{d\tau} \psi^{(1)} &= - H^{(1)}(\theta^{(1)}) \psi^{(1)} + \epsilon\, G_{12}(\psi^{(2)}), \\
    \frac{d}{d\tau} \psi^{(2)} &= - H^{(2)}(\theta^{(2)}) \psi^{(2)} + \epsilon\, G_{21}(\psi^{(1)}),
\end{align}
where $G_{12}, G_{21}$ are interaction terms and $\epsilon$ is a small coupling constant. Even when $G_{12}$ and $G_{21}$ arise from mundane mechanisms (e.g., sharing gradients on overlapping data or predicting each other's outputs), the complex representation space allows the models' phases to partially synchronize.

We can view distillation or alignment as a form of \emph{phase locking} between models: their internal phase modes become correlated, leading to more compatible or interoperable behaviors.

\subsection{Model--human coupling}

Human users interacting with LLMs can be seen as providing a sequence of observations and feedback that constrain the model's state trajectory. While human cognition is not literally complex-valued, we can abstract both the human and the model as systems that maintain internal representations and update them in response to observations.

In this view:
\begin{itemize}
    \item The model's Hilbert-space state encodes its beliefs and predictions in complex amplitude-phase form.
    \item Human prompts and feedback shape the model's loss landscape, thereby modifying the imaginary-time gradient flow and nudging internal phases toward configurations that better match human expectations.
    \item Over repeated interactions, a given user and model may develop idiosyncratic ``interaction modes''---patterns of prompts and responses that correspond to particular regions of the model's Hilbert space and recurrently induced phase structures.
\end{itemize}

While speculative, this perspective suggests concrete analyses:
\begin{itemize}
    \item Measure how fine-tuning on a particular user's data modifies the spectral structure of a Hilbert-space transformer (e.g., eigenvalues of learned generators, statistics of phase differences).
    \item Track whether long-term personalization leads to stable, low-dimensional manifolds of phase-locked states associated with that user.
\end{itemize}

At a minimum, the Hilbert-space formulation provides a language for discussing model--human co-adaptation that emphasizes \emph{resonance} (matched modes) rather than only parameter shifts.

\section{Experimental Program}

We outline an experimental agenda rather than reporting results.

\subsection{Toy and medium-scale experiments}

\paragraph{Char-level language modeling.} Train real-valued and Hilbert-space transformers of comparable parameter count on enwik8 or text8. Compare bits-per-character, training stability, and gradient norm statistics, particularly as depth and context length increase.

\paragraph{Long-context synthetic tasks.} Evaluate on synthetic tasks such as copying, addition, and nested bracket matching at varying sequence lengths. Test whether near-unitary complex flows better preserve information over long horizons than real-valued baselines.

\paragraph{Phase analysis.} For a trained Hilbert-space transformer, analyze:
\begin{itemize}
    \item Phase distributions of activations across layers and heads.
    \item Measures of phase-locking, such as coherence between layers for specific dimensions or head subspaces.
    \item Correlations between phase patterns and token types, positions, or attention patterns.
\end{itemize}

\subsection{Scaling toward GPT-like regimes}

\paragraph{Mid-scale LLMs.} Train mid-sized (1--10B parameter) Hilbert-space transformers on standard text corpora. Compare against similarly sized real-valued transformers on:
\begin{itemize}
    \item Language modeling metrics (perplexity, negative log-likelihood).
    \item Reasoning and understanding benchmarks (e.g., MMLU, GSM8K).
    \item Long-context benchmarks (e.g., retrieval from long documents).
\end{itemize}

\paragraph{Ablations.} Explore:
\begin{itemize}
    \item Fully complex vs.\ partially complex models (e.g., complex attention, real FFN).
    \item With vs.\ without near-unitary parameterization.
    \item Different activation families (split vs.\ amplitude-phase).
\end{itemize}

\paragraph{Robustness and generalization.} Evaluate sensitivity to input noise and distribution shift. Investigate whether complex-valued structure improves robustness in the presence of adversarial perturbations or limited training data.

\section{Discussion and Open Problems}

\paragraph{Compute and hardware.} Complex matrix multiplications are more expensive than real ones, typically costing between $2\times$ and $4\times$ as many FLOPs and increased memory bandwidth if implemented naively. Efficient complex kernels or hardware that natively supports complex arithmetic (e.g., analog or optical accelerators) could mitigate this overhead.

\paragraph{Activation design.} Complex analysis introduces constraints on analytic, bounded activations (e.g., Liouville's theorem). Practical CVNNs often rely on split activations or amplitude-phase functions that are not holomorphic but are still trainable in the Wirtinger framework~\citep{trabelsi2018deepcomplex}. Understanding the trade-offs between mathematical elegance and empirical performance remains an open problem.

\paragraph{Interpretability.} The Hilbert-space view encourages spectral and phase-based tools for interpretability. For example, one could analyze learned generators $H_\ell$ via their eigenvalues and eigenvectors, or study how particular spectral components correspond to recognizable behaviors (e.g., syntactic structure, world knowledge, style).

\paragraph{Bridging to quantum and analog hardware.} Complex Hilbert-space formulations align more naturally with quantum representations and some analog computing paradigms. While our proposal is entirely classical, it may offer a cleaner conceptual bridge to future hybrid architectures.

\section{Conclusion}

We have proposed a framework for training transformer-based large language models directly in complex Hilbert space, using Wirtinger calculus and an imaginary-time gradient flow perspective. Our Hilbert-space transformer architecture combines complex-valued attention, near-unitary parameterizations, and phase-aware representations to more fully exploit the wave-like, interference-based nature of deep sequence models. Existing work on complex-valued neural networks, unitary RNNs, and complex transformers strongly suggests that such models are trainable and can offer practical advantages; the next step is to validate these ideas at GPT scale.

Beyond architecture and optimization details, the resonant phase-locking perspective offers a way to think about the internal organization of large models---and their coupling to each other and to humans---in terms of synchronized modes in a shared abstract space. Whether this leads to measurably better performance, robustness, or controllability is an empirical question, but one that can now be posed precisely.

\bigskip

\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML)}, 2016.

\bibitem[Eilers and Jiang(2023)]{eilers2023complextransformer}
Florian Eilers and Xiaoyi Jiang.
\newblock Building blocks for a complex-valued transformer architecture.
\newblock \emph{arXiv preprint arXiv:2306.09827}, 2023.

\bibitem[Helfrich et~al.(2017)Helfrich, Willmott, and Ye]{helfrich2017orthogonalrnn}
Kevin~E. Helfrich, Daniel Willmott, and Qiang Ye.
\newblock Orthogonal recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09520}, 2017.

\bibitem[Maduranga and Lu(2019)]{maduranga2019cayley}
Kasun~D. G.~Maduranga and Yao Lu.
\newblock Complex unitary recurrent neural networks using scaled Cayley
  transform.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Trabelsi et~al.(2018)Trabelsi et~al.]{trabelsi2018deepcomplex}
Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep
  Subramanian, Jo{\~a}o~Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua
  Bengio, and Christopher~J. Pal.
\newblock Deep complex networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Wisdom et~al.(2016)Wisdom et~al.]{wisdom2016fullcapacity}
Scott Wisdom, Thomas Powers, John~R. Hershey, Jonathan Le~Roux, and Les~Atlas.
\newblock Full-capacity unitary recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2016.

\bibitem[Wolter and Yao(2019)]{wolter2019complexgru}
Moritz Wolter and Angela Yao.
\newblock Complex gated recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Yang et~al.(2019)Yang et~al.]{yang2019complextransformer}
Muqiao Yang, Martin~Q. Ma, Dongyu Li, Yao-Hung~Hubert Tsai, and Ruslan
  Salakhutdinov.
\newblock Complex transformer: A framework for modeling complex-valued
  sequence.
\newblock In \emph{ICASSP 2020 - IEEE International Conference on Acoustics,
  Speech and Signal Processing}, 2020.

\bibitem[Zhang et~al.(2025)Zhang et~al.]{unveiling2025complexvalued}
(Example) X.~Zhang, Y.~Liu, and J.~Wang.
\newblock Unveiling the power of complex-valued transformers in wireless
  communications.
\newblock \emph{arXiv preprint arXiv:2502.11151}, 2025.

\end{thebibliography}

\end{document}
